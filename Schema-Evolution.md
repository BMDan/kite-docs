---
layout: page
title: Schema Evolution
---

Over time, you might want to add or remove fields in an existing schema. The precise rules for schema evolution are inherited from Avro, and are documented in the Avro specification as rules for [Avro schema resolution][]. For the purposes of working in Kite, here are some important things to note.

## Writer Schemas and Reader Schemas

Writer schemas describe a dataset as it is being written. Reader schemas describe a dataset as it is being read from a datastore. Writer and reader schemas must be compatible, but they do not have to match exactly. See the [Avro schema resolution][] specification for the exhaustive list of rules for matching one schema to another.

## Changing Field Types

You can use schema resolution to change the type used to store a value. For example, you can change an `int` to a `long` to handle values that grow larger than initially anticipated.

## Removing Fields from a Dataset

When you remove fields from a dataset schema, the data already written remains unchanged. The fields you remove are not required when records are written going forward. The field must not be added back, unless it is identical to the existing field (since the data isn't actually removed from the dataset).

## Adding Fields to a Dataset

You can add fields to a dataset's schema, provided the the schema is compatible with the existing data. If you do so, you must define a default value for the fields you add to the dataset schema. New data that includes the field will be populated normally. Records that do not include the field are populated with the default you provide.

## Reading with Different Schemas

You can have a schema that reads fewer fields than are defined by the schema used to write a dataset, provided that the field definitions in the reader schema are compatible with the chosen fields in the writer schema. This is useful when the writer schema provides more fields than are needed for the business case supported by the reader schema.

Removing unnecessary fields allows Kite to read data more efficiently. The performance gain can be significant when using Parquet format, in particular.

Kite ensures that each change to a schema is compatible with the last version of the schema. Older data can always be read by the current schema.

## Example of Schema Evolution

Here's a painfully simple example that demonstrates how to update the schema for a dataset about flowers using the Kite CLI. Complete sample code is available in [flowers.tar.gz](../samples/flowers.tar.gz).

Begin with a CSV data file (flowers.csv).

```
id,name
1,Rose
2,Tulip
3,Marigold
```

Generate an Avro schema file (`flowers.avsc`) using `flowers.csv`.

```
$ dataset csv-schema flowers.csv --class flowers -o flowers.avsc
```

The schema `flowers.avsc` describes fields for _id_ number and the _name_ of the flower.

```
{
  "type" : "record",
  "name" : "Flowers",
  "doc" : "Schema generated by Kite",
  "fields" : [ {
    "name" : "id",
    "type" : [ "null", "long" ],
    "doc" : "Type inferred from '1'"
  }, {
    "name" : "name",
    "type" : [ "null", "string" ],
    "doc" : "Type inferred from 'Rose'"
  } ]
}
```


Create the _flowers_ dataset.

```
$ dataset create flowers --schema flowers.avsc
```

Import the CSV data.

```
$ dataset csv-import flowers.csv flowers
```

Validate the dataset by showing the first few records.

```
$ dataset show flowers
{"id": 1, "name": "Rose"}
{"id": 2, "name": "Tulip"}
{"id": 3, "name": "Marigold"}
```

Now that you've created your dataset, you immediately receive a request from your florist to add a field for what the flower symbolizes. Sigh. You can modify the Avro schema file to add the _symbol_ field. When you add a new field, you must provide a default value that is used to fill in the field for existing records.

In this case, the default value is _null_. Note that you don't put quotation marks around _null_ when setting it as the default value.

The source code for this file is `flowers2.avsc`.

```
{
  "type" : "record",
  "name" : "Flowers",
  "doc" : "Schema generated by Kite",
  "fields" : [ {
    "name" : "id",
    "type" : [ "null", "long" ],
    "doc" : "Type inferred from '1'"
  }, {
    "name" : "name",
    "type" : [ "null", "string" ],
    "doc" : "Type inferred from 'Rose'"
  }, {
    "name" : "symbol",
    "type" : [ "null", "string" ],
    "default" : null,
    "doc" : "Manually added field."
  } ]
}
```

Use the CLI `update` command to add the new field.

```
$ dataset update flowers --schema flowers2.avsc
```

Now you can load more records that include values for the _symbol_ field.  These records are in the file `flowers2.csv`.
 
```
id,name,symbol
4,daisy,Innocence
5,Snapdragon,Strength
6,Violet,Renewal

```

After you import `flowers2.csv`, the existing records display _null_ for the _symbol_ field.

```
$ dataset csv-import flowers2.csv flowers
Added 3 records to "flowers"
$ dataset show flowers
{"id": 1, "name": "Rose", "symbol": null}
{"id": 2, "name": "Tulip", "symbol": null}
{"id": 3, "name": "Marigold", "symbol": null}
{"id": 4, "name": "Daisy", "symbol": "Innocence"}
{"id": 5, "name": "Snapdragon", "symbol": "Strength"}
{"id": 6, "name": "Violet", "symbol": "Renewal"}
```

What a complete and satisfying flowers dataset. But not so fast. Your florist realizes that the _name_ field categorization is too broad, and wants to use decimals to identify subspecies within each of the _name_ groups (just go with me here, the point is you're going to change the _id_ field from a long integer to a double).

Update the schema definition, changing the _id_ field datatype from _long_ to _double_. The source code for this file is `flowers3.avsc`.

```
{
  "type" : "record",
  "name" : "Flowers",
  "doc" : "Schema generated by Kite",
  "fields" : [ {
    "name" : "id",
    "type" : [ "null", "double" ],
    "doc" : "Datatype changed manually.'"
  }, {
    "name" : "name",
    "type" : [ "null", "string" ],
    "doc" : "Type inferred from 'Rose'"
  }, {
    "name" : "symbol",
    "type" : [ "null", "string" ],
    "default" : null,
    "doc" : "Manually added field."
  } ]
}
```

Once again, update the schema.

```
$ dataset update flowers --schema flowers3.avsc
```
If you run the `show` command, you'll see that the existing integer _id_ field values now display values with a decimal point and 0.

```
$ dataset show flowers
{"id": 1.0, "name": "Rose", "symbol": null}
{"id": 2.0, "name": "Tulip", "symbol": null}
{"id": 3.0, "name": "Marigold", "symbol": null}
{"id": 4.0, "name": "Daisy", "symbol": "Innocence"}
{"id": 5.0, "name": "Snapdragon", "symbol": "Strength"}
{"id": 6.0, "name": "Violet", "symbol": "Renewal "}

```
The current _id_ values are small, and could easily fit into a _float_ datatype. However, the current datatype is _long_. You have to convert the field to a _double_ datatype, because the highest potential value in a _long_ integer is too high to store in a _float_ field.

The datafile `flowers3.csv` contains records that are varieties of roses, with decimal _id_ numbers.

```
id,name,symbol
1.1,Rambling Rose,Travel
1.2,Yellow Rose,Texas
1.3,Gypsy Rose,Transparency
```

Import the records and show the table.

```
$ dataset csv-import flowers3.csv flowers
Added 3 records to "flowers"
$ dataset show flowers
{"id": 1.1, "name": "Rambling Rose", "symbol": "Travel"}
{"id": 1.2, "name": "Yellow Rose", "symbol": "Texas"}
{"id": 1.3, "name": "Gypsy Rose", "symbol": "Transparency"}
{"id": 1.0, "name": "Rose", "symbol": null}
{"id": 2.0, "name": "Tulip", "symbol": null}
{"id": 3.0, "name": "Marigold", "symbol": null}
{"id": 4.0, "name": "Daisy", "symbol": "Innocence"}
{"id": 5.0, "name": "Snapdragon", "symbol": "Strength"}
{"id": 6.0, "name": "Violet", "symbol": "Renewal "}
```

See [Avro schema resolution][] for further options.

[Avro Schema resolution]: http://avro.apache.org/docs/current/spec.html#Schema+Resolution "schemaSpec"
